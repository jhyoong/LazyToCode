LAZYTOCODE IMPLEMENTATION PLAN
==============================

Project: AI Coding Agent using Microsoft Autogen 0.6.1+
Date: June 23, 2025
Based on requirements from CLAUDE.md

OVERVIEW
--------
Create a CLI tool that uses Microsoft's Autogen library to build an AI agent capable of writing code based on user prompts. The agent will support multiple model providers (Ollama, LlamaCpp) and generate code to specified output directories.

SYSTEM REQUIREMENTS
-------------------
- Python 3.10+
- Microsoft Autogen 0.6.1+
- Ollama (optional, for local model hosting)
- LlamaCpp (optional, for GGUF model support)

PROJECT STRUCTURE
------------------
LazyToCode/
├── main.py                 # CLI entry point with async support
├── requirements.txt        # Python dependencies
├── .env.example           # Environment configuration template
├── .env                   # User environment file (gitignored)
├── config/
│   └── agent_config.py    # Model client configuration and factory
├── agents/
│   ├── __init__.py        # Package initialization
│   ├── coding_assistant.py # AssistantAgent for code generation
│   └── user_proxy.py      # UserProxyAgent for task management
├── utils/
│   ├── __init__.py        # Package initialization
│   ├── cli_parser.py      # Command line argument parsing
│   ├── file_handler.py    # Async file I/O operations
│   └── logger.py          # Logging configuration
└── output/                # Default output directory for generated code

DEPENDENCIES (requirements.txt)
-------------------------------
autogen-core>=0.6.1
autogen-ext[ollama]>=0.6.1
autogen-ext[llama-cpp]>=0.6.1
python-dotenv>=1.0.0

CLI INTERFACE SPECIFICATION
---------------------------
Command: python main.py [OPTIONS]

Required Arguments:
- --prompt TEXT|FILE       : Text prompt or path to .txt file containing prompt

Optional Arguments:
- --output_dir PATH        : Output directory for generated code (default: ./output)
- --model TEXT             : Model name (default: Qwen2.5-Coder)
- --model_provider TEXT    : Provider choice - 'ollama' or 'llamacpp' (default: ollama)
- --debug                  : Enable verbose logging (flag)
- --help                   : Show help message

Examples:
python main.py --prompt "Create a Python function to calculate fibonacci numbers"
python main.py --prompt ./prompts/web_scraper.txt --output_dir ./my_code --model llama3
python main.py --prompt "Build a REST API" --model_provider llamacpp --debug

IMPLEMENTATION DETAILS
-----------------------

1. ENVIRONMENT CONFIGURATION (.env.example)
   OLLAMA_ENDPOINT=http://localhost:11434
   OLLAMA_MODEL=Qwen2.5-Coder
   
   LLAMACPP_MODEL_PATH=/path/to/model.gguf
   LLAMACPP_REPO_ID=unsloth/Qwen2.5-Coder-7B-Instruct-GGUF
   LLAMACPP_FILENAME=Qwen2.5-Coder-7B-Instruct-Q4_K_M.gguf
   LLAMACPP_N_GPU_LAYERS=-1
   LLAMACPP_N_CTX=4096
   
   DEFAULT_OUTPUT_DIR=./output
   DEBUG_MODE=false

2. CLI PARSER (utils/cli_parser.py)
   - Use argparse for command line processing
   - Validate required arguments
   - Handle file vs string prompt input
   - Set up default values from environment

3. MODEL CLIENT FACTORY (config/agent_config.py)
   - Abstract factory pattern for model clients
   - Ollama client configuration with host/model settings
   - LlamaCpp client with local file or HuggingFace model support
   - Environment-based configuration loading
   - Connection validation and error handling

4. CODING ASSISTANT AGENT (agents/coding_assistant.py)
   - Extend AssistantAgent from autogen-core
   - System message for code generation instructions
   - Output directory management
   - Code quality and documentation guidelines
   - Support for multiple programming languages

5. USER PROXY AGENT (agents/user_proxy.py)
   - Extend UserProxyAgent for task coordination  
   - Handle user input and prompt processing
   - Manage conversation flow
   - File writing coordination

6. FILE OPERATIONS (utils/file_handler.py)
   - Async file reading for prompt files
   - Output directory creation and validation
   - Generated code file writing
   - File permission handling
   - Backup and versioning support

7. LOGGING SYSTEM (utils/logger.py)
   - Configurable logging levels
   - Debug mode verbose output
   - File and console logging
   - Agent interaction tracking

8. MAIN APPLICATION (main.py)
   - Async main function using asyncio
   - CLI argument parsing and validation
   - Environment setup and configuration
   - Model client initialization
   - Agent creation and coordination
   - Error handling and graceful shutdown

AGENT WORKFLOW
--------------
1. Parse CLI arguments and load environment
2. Initialize model client based on provider
3. Create CodingAssistant and UserProxy agents
4. Process prompt (file or text)
5. Execute agent conversation for code generation
6. Write generated code to output directory
7. Log results and cleanup

ERROR HANDLING STRATEGY
-----------------------
- Model provider connectivity validation
- Output directory permission checks
- File I/O error management
- Graceful degradation for network issues
- User-friendly error messages
- Debug mode for detailed troubleshooting

SECURITY CONSIDERATIONS
-----------------------
- No hardcoded secrets or API keys
- Environment-based configuration
- Output directory sandboxing
- Input validation and sanitization
- Safe file writing operations

TESTING STRATEGY
----------------
- Unit tests for each component
- Integration tests for agent workflows
- CLI argument validation tests
- Mock model client for testing
- File I/O operation tests

DEPLOYMENT CONSIDERATIONS
-------------------------
- Virtual environment setup instructions
- Docker containerization option
- Model download and setup guides
- Configuration validation scripts
- Performance monitoring hooks

FUTURE ENHANCEMENTS
-------------------
- Support for additional model providers
- Code review and quality checking agents
- Multi-file project generation
- Template-based code generation
- Interactive conversation mode
- Web interface option
- Plugin system for custom agents

IMPLEMENTATION PHASES
--------------------
Phase 1: Core Infrastructure
- Project structure setup
- CLI interface and argument parsing
- Environment configuration
- Basic logging system

Phase 2: Model Integration  
- Ollama client implementation
- LlamaCpp client implementation
- Model factory and configuration

Phase 3: Agent Development
- CodingAssistant agent
- UserProxy agent
- Agent coordination logic

Phase 4: File Operations
- Prompt processing
- Code generation output
- File management utilities

Phase 5: Testing & Polish
- Unit and integration tests
- Error handling improvements
- Documentation and examples
- Performance optimization

VALIDATION CHECKLIST
--------------------
□ CLI accepts all required arguments
□ Environment configuration loads correctly
□ Ollama model client connects and generates code
□ LlamaCpp model client works with local/remote models
□ Generated code writes to specified output directory
□ Debug logging provides useful information
□ Error handling covers common failure cases
□ File operations are safe and robust
□ Async operations work correctly
□ Agent conversation flows naturally

END OF IMPLEMENTATION PLAN
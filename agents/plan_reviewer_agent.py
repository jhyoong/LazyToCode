"""
Plan Reviewer Agent - Implements the Reflection design pattern for plan improvement.

This agent reviews implementation plans generated by the Planner Agent and provides
detailed feedback for iterative improvement, following the Microsoft Autogen Reflection pattern.
"""

from typing import Optional, Dict, Any, List
from pathlib import Path
import json
from datetime import datetime
import uuid

# Import from autogen-core
from autogen_core.models import UserMessage, SystemMessage

from agents.base_agent import BaseAgent
from utils.agent_messages import (
    AgentMessage, AgentStatus, MessageType,
    create_status_update, create_error_report
)
from utils.logger import get_logger


class PlanReviewerAgent(BaseAgent):
    """AI Plan Reviewer Agent that critiques and improves implementation plans using reflection."""
    
    def __init__(self, 
                 name: str = "PlanReviewerAgent",
                 model_client=None,
                 output_dir: Optional[Path] = None,
                 debug_mode: bool = False,
                 **kwargs):
        
        super().__init__(name, "plan_reviewer", **kwargs)
        
        self.model_client = model_client
        self.output_dir = output_dir or Path("./output")
        self.debug_mode = debug_mode
        
        # Create debug directory if debug mode is enabled
        if self.debug_mode:
            self.debug_dir = self.output_dir / "debug"
            self.debug_dir.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"Debug mode enabled. Debug files will be saved to: {self.debug_dir}")
        
        # Plan Reviewer-specific capabilities
        self.capabilities = [
            "plan_analysis",
            "plan_critique", 
            "improvement_suggestions",
            "plan_validation",
            "reflection_feedback"
        ]
        
        # System message for the plan reviewer
        self.system_message = self._create_system_message()
        
        self.logger.info(f"PlanReviewerAgent '{name}' initialized with debug_mode: {debug_mode}")
    
    def _create_system_message(self) -> str:
        """Create system message for the plan reviewer agent."""
        
        return """You are an expert implementation plan reviewer specialized in software development project analysis. Your role is to critically evaluate implementation plans and provide detailed, actionable feedback for improvement using the Reflection design pattern.

CORE RESPONSIBILITIES:
1. Analyze implementation plans for completeness, feasibility, and best practices
2. Identify potential issues, gaps, and improvement opportunities
3. Provide structured feedback with specific suggestions
4. Evaluate technical decisions and architecture choices
5. Assess project complexity and phase breakdown appropriateness
6. Question the plan for core files necessary for build success in the project.

PLAN REVIEW CRITERIA (in order of importance):
- **Completeness**: Are all necessary components and files included in the entire project?
- **Feasibility**: Is the plan realistic and achievable?
- **Structure**: Is the phase breakdown logical and dependencies clear?
- **Best Practices**: Does the plan follow modern development standards?
- **Technology Choices**: Are the selected technologies appropriate?
- **Testing Strategy**: Is testing adequately planned?
- **Documentation**: Is documentation approach sufficient?
- **Success Criteria**: Are success criteria clear and measurable?

FEEDBACK STRUCTURE:
Provide feedback in the following JSON format:
{
    "overall_assessment": {
        "score": 1-10,
        "summary": "brief overall assessment",
        "recommendation": "approve_with_minor_changes" | "needs_major_revision" | "approve_as_is"
    },
    "strengths": [
        "specific strength 1",
        "specific strength 2"
    ],
    "issues": [
        {
            "category": "completeness" | "feasibility" | "best_practices" | "structure" | "technology",
            "severity": "low" | "medium" | "high" | "critical",
            "description": "detailed issue description",
            "affected_areas": ["phase_1", "dependencies", "overall_structure"],
            "suggestion": "specific improvement suggestion"
        }
    ],
    "improvements": [
        {
            "area": "phase breakdown" | "technology choice" | "testing" | "documentation" | "structure",
            "priority": "high" | "medium" | "low",
            "description": "what to improve",
            "specific_changes": "detailed change instructions"
        }
    ],
    "questions": [
        "clarifying question 1",
        "clarifying question 2"
    ]
}

REVIEW PRINCIPLES:
- Be constructive and specific in feedback
- Focus on actionable improvements
- Consider project context and complexity
- Evaluate technical feasibility
- Always ask if there are any key files missing from the project
- Consider security and performance implications
- Validate success criteria alignment with deliverables

CRITICAL EVALUATION AREAS:
1. Phase dependencies and logical flow
2. File organization and project structure
3. Technology stack appropriateness
4. Testing strategy completeness
5. Documentation and setup requirements
6. Success criteria measurability
7. There cannot be any missing Key files in the overall structure

Output detailed, constructive feedback that helps improve the implementation plan quality.
"""
    
    async def review_plan(self, plan_dict: Dict[str, Any], review_context: Optional[Dict] = None) -> Dict[str, Any]:
        """
        Review an implementation plan and provide detailed feedback.
        
        Args:
            plan_dict: The plan dictionary to review
            review_context: Optional context for the review
            
        Returns:
            Dictionary containing review feedback and assessment
        """
        
        try:
            self.logger.info("Starting plan review process")
            self.set_status(AgentStatus.WORKING, "Reviewing implementation plan")
            
            # Create review prompt
            review_prompt = self._create_review_prompt(plan_dict, review_context)
            
            # Call the model to generate the review
            review_response = await self._process_review_prompt(review_prompt)
            
            # Parse and validate the review
            review_feedback = self._parse_and_validate_review(review_response)
            
            # Save review to file for reference
            await self._save_review_to_file(review_feedback, plan_dict)
            
            self.set_status(AgentStatus.COMPLETED, "Plan review completed")
            self.logger.info("Plan review completed successfully")
            
            return {
                "success": True,
                "review_feedback": review_feedback,
                "reviewed_plan": plan_dict
            }
            
        except Exception as e:
            self.logger.error(f"Failed to review plan: {e}")
            self.set_status(AgentStatus.ERROR, f"Plan review failed: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "review_feedback": None
            }
    
    def _create_review_prompt(self, plan_dict: Dict[str, Any], review_context: Optional[Dict] = None) -> str:
        """
        Create a prompt for reviewing an implementation plan.
        
        Args:
            plan_dict: The plan dictionary to review
            review_context: Optional context for the review
            
        Returns:
            Formatted review prompt
        """
        
        # Extract plan details
        project_info = plan_dict.get("project_info", {})
        phases = plan_dict.get("phases", [])
        overall_structure = plan_dict.get("overall_structure", {})
        
        prompt = f"""
TASK: Review the following implementation plan and provide detailed feedback.

PROJECT INFORMATION:
- Project Name: {project_info.get('name', 'Unknown')}
- Project Type: {project_info.get('type', 'Unknown')}
- Language: {project_info.get('language', 'Unknown')}
- Complexity: {project_info.get('complexity', 'Unknown')}
- Description: {project_info.get('description', 'Not provided')}

IMPLEMENTATION PLAN TO REVIEW:
{json.dumps(plan_dict, indent=2)}

REVIEW CONTEXT:
"""
        
        if review_context:
            prompt += f"{json.dumps(review_context, indent=2)}\n"
        else:
            prompt += "No additional context provided.\n"
        
        prompt += """
REVIEW INSTRUCTIONS:
1. Carefully analyze the implementation plan against the review criteria
2. Evaluate each phase for completeness, feasibility, and best practices
3. Assess the overall project structure and technology choices
4. Identify potential issues and improvement opportunities
5. Provide specific, actionable feedback

Focus on:
- Technical feasibility and best practices
- Phase breakdown and dependencies
- File organization and project structure
- Testing strategy and documentation approach
- Success criteria clarity and measurability
- Technology stack appropriateness
- Risk identification and mitigation strategies

Provide your review in the specified JSON format with detailed, constructive feedback.
"""
        
        return prompt
    
    async def _process_review_prompt(self, prompt: str) -> str:
        """Process the review prompt using the model client."""
        
        session_id = uuid.uuid4().hex[:12]
        
        try:
            self.logger.debug(f"Processing review prompt with session ID: {session_id}")
            
            if self.model_client is None:
                raise ValueError("Model client not initialized")
            
            # Create the messages for the chat completion
            messages = [
                SystemMessage(content=self.system_message, source="system"),
                UserMessage(content=prompt, source="user")
            ]
            
            # Log the request if debug mode is enabled
            if self.debug_mode:
                await self._log_debug_request(session_id, messages, prompt)
            
            # Call the model client to generate a response
            self.logger.debug("Calling model client for plan review")
            start_time = datetime.now()
            
            try:
                response = await self.model_client.create(messages)
                end_time = datetime.now()
                duration = (end_time - start_time).total_seconds()
                self.logger.debug(f"Model client call completed for review in {duration:.2f}s")
            except Exception as model_error:
                self.logger.error(f"Model client call failed: {model_error}")
                if self.debug_mode:
                    await self._log_debug_error(session_id, str(model_error))
                raise
            
            # Extract content from CreateResult
            if hasattr(response, 'content'):
                content = response.content
                self.logger.debug(f"Generated review content length: {len(content)} characters")
                
                # Log the response if debug mode is enabled
                if self.debug_mode:
                    await self._log_debug_response(session_id, response, content, duration)
                
                return content
            else:
                error_msg = f"Could not extract content from response: {response}"
                self.logger.error(error_msg)
                if self.debug_mode:
                    await self._log_debug_error(session_id, error_msg)
                raise ValueError(error_msg)
                
        except Exception as e:
            self.logger.error(f"Error processing review prompt: {e}")
            if self.debug_mode:
                await self._log_debug_error(session_id, str(e))
            raise
    
    def _parse_and_validate_review(self, review_response: str) -> Dict[str, Any]:
        """Parse and validate the generated review feedback."""
        
        try:
            # Extract JSON from the response (handle markdown code blocks)
            json_content = self._extract_json_from_response(review_response)
            
            # Check if we actually got JSON-like content
            if not json_content.strip().startswith('{'):
                self.logger.warning("Review response does not contain valid JSON structure")
                # Return a basic structured response
                return self._create_fallback_review(review_response)
            
            # Parse JSON
            try:
                review = json.loads(json_content)
            except json.JSONDecodeError as json_err:
                self.logger.warning(f"JSON parsing failed: {json_err}")
                return self._create_fallback_review(review_response)
            
            # Validate review structure
            self._validate_review_structure(review)
            
            # Add metadata
            review["metadata"] = {
                "reviewed_at": datetime.now().isoformat(),
                "reviewer_agent": self.name,
                "version": "1.0"
            }
            
            return review
            
        except Exception as e:
            self.logger.warning(f"Failed to parse review response: {e}")
            return self._create_fallback_review(review_response)
    
    def _extract_json_from_response(self, response: str) -> str:
        """Extract JSON content from model response, handling markdown code blocks."""
        
        # Remove markdown code blocks if present
        response = response.strip()
        
        # Look for JSON content between ```json and ``` or ``` and ```
        if "```json" in response:
            start = response.find("```json") + 7
            end = response.find("```", start)
            if end != -1:
                return response[start:end].strip()
        elif "```" in response:
            start = response.find("```") + 3
            end = response.find("```", start)
            if end != -1:
                return response[start:end].strip()
        
        # If no code blocks, try to find JSON directly
        # Look for the first { and last }
        start = response.find("{")
        end = response.rfind("}")
        
        if start != -1 and end != -1 and end > start:
            return response[start:end+1]
        
        # If no JSON found, return the whole response
        return response
    
    def _validate_review_structure(self, review: Dict[str, Any]):
        """Validate that the review has the required structure."""
        
        required_keys = ["overall_assessment", "strengths", "issues", "improvements"]
        for key in required_keys:
            if key not in review:
                self.logger.warning(f"Review missing recommended key: {key}")
        
        # Validate overall_assessment if present
        if "overall_assessment" in review:
            assessment = review["overall_assessment"]
            if not isinstance(assessment, dict):
                review["overall_assessment"] = {"summary": str(assessment)}
    
    def _create_fallback_review(self, review_response: str) -> Dict[str, Any]:
        """Create a fallback review structure when parsing fails."""
        
        return {
            "overall_assessment": {
                "score": 7,
                "summary": "Review completed but response parsing failed",
                "recommendation": "needs_major_revision"
            },
            "strengths": [
                "Plan structure appears logical"
            ],
            "issues": [
                {
                    "category": "review_parsing",
                    "severity": "medium",
                    "description": "Unable to parse detailed review feedback",
                    "affected_areas": ["overall"],
                    "suggestion": "Manual review recommended"
                }
            ],
            "improvements": [
                {
                    "area": "overall",
                    "priority": "medium",
                    "description": "Review process needs improvement",
                    "specific_changes": "Check model response format"
                }
            ],
            "questions": [
                "Is the plan structure appropriate for the project requirements?"
            ],
            "raw_response": review_response,
            "metadata": {
                "reviewed_at": datetime.now().isoformat(),
                "reviewer_agent": self.name,
                "version": "1.0",
                "fallback_used": True
            }
        }
    
    async def _save_review_to_file(self, review_feedback: Dict[str, Any], original_plan: Dict[str, Any]):
        """Save the review feedback to a JSON file."""
        
        try:
            # Ensure output directory exists
            self.output_dir.mkdir(parents=True, exist_ok=True)
            
            # Generate filename with timestamp (use different prefix to avoid Writer Agent confusion)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"review_feedback_{timestamp}.json"
            filepath = self.output_dir / filename
            
            # Combine review with original plan for context
            review_data = {
                "review_feedback": review_feedback,
                "original_plan": original_plan,
                "review_metadata": {
                    "timestamp": timestamp,
                    "reviewer": self.name
                }
            }
            
            # Save review to file
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(review_data, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"Review saved to: {filepath}")
            
        except Exception as e:
            self.logger.warning(f"Could not save review to file: {e}")
    
    async def _log_debug_request(self, session_id: str, messages: List, prompt: str):
        """Log debug information for the model request."""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            debug_file = self.debug_dir / f"review_request_{session_id}_{timestamp}.json"
            
            debug_data = {
                "session_id": session_id,
                "timestamp": datetime.now().isoformat(),
                "agent_name": self.name,
                "request_type": "plan_review",
                "original_prompt": prompt,
                "system_message_length": len(self.system_message),
                "messages": [
                    {
                        "role": getattr(msg, 'source', getattr(msg, 'role', 'unknown')),
                        "content_length": len(msg.content),
                        "content_preview": msg.content[:200] + "..." if len(msg.content) > 200 else msg.content
                    } for msg in messages
                ]
            }
            
            with open(debug_file, 'w', encoding='utf-8') as f:
                json.dump(debug_data, f, indent=2, ensure_ascii=False)
            
            self.logger.debug(f"Debug request logged to: {debug_file}")
            
        except Exception as e:
            self.logger.warning(f"Failed to log debug request: {e}")
    
    async def _log_debug_response(self, session_id: str, response: Any, content: str, duration: float):
        """Log debug information for the model response."""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            debug_file = self.debug_dir / f"review_response_{session_id}_{timestamp}.json"
            
            debug_data = {
                "session_id": session_id,
                "timestamp": datetime.now().isoformat(),
                "agent_name": self.name,
                "response_type": "plan_review",
                "duration_seconds": duration,
                "content_length": len(content),
                "full_response_content": content,
                "content_preview": content[:500] + "..." if len(content) > 500 else content
            }
            
            with open(debug_file, 'w', encoding='utf-8') as f:
                json.dump(debug_data, f, indent=2, ensure_ascii=False)
            
            self.logger.debug(f"Debug response logged to: {debug_file}")
            
        except Exception as e:
            self.logger.warning(f"Failed to log debug response: {e}")
    
    async def _log_debug_error(self, session_id: str, error_message: str):
        """Log debug information for errors."""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            debug_file = self.debug_dir / f"review_error_{session_id}_{timestamp}.json"
            
            debug_data = {
                "session_id": session_id,
                "timestamp": datetime.now().isoformat(),
                "agent_name": self.name,
                "error_type": "plan_review_error",
                "error_message": error_message
            }
            
            with open(debug_file, 'w', encoding='utf-8') as f:
                json.dump(debug_data, f, indent=2, ensure_ascii=False)
            
            self.logger.debug(f"Debug error logged to: {debug_file}")
            
        except Exception as e:
            self.logger.warning(f"Failed to log debug error: {e}")
    
    # Implementation of abstract methods from BaseAgent
    
    async def _handle_plan_request(self, message: AgentMessage) -> Optional[AgentMessage]:
        """Handle plan review request messages."""
        
        try:
            payload = message.payload
            plan_dict = payload.get('plan_dict')
            review_context = payload.get('review_context')
            
            if not plan_dict:
                raise ValueError("Plan review request missing plan_dict")
            
            # Review the plan
            review_result = await self.review_plan(plan_dict, review_context)
            
            # Create response message
            response = AgentMessage(
                message_type=MessageType.PLAN_RESPONSE,
                sender=self.name,
                recipient=message.sender,
                payload=review_result,
                phase_id=message.phase_id,
                timestamp=datetime.now(),
                correlation_id=message.correlation_id
            )
            
            return response
            
        except Exception as e:
            self.logger.error(f"Error handling plan review request: {e}")
            error_msg = create_error_report(
                sender=self.name,
                recipient=message.sender,
                error=str(e),
                error_type="plan_review_error",
                phase_id=message.phase_id,
                correlation_id=message.correlation_id
            )
            return error_msg
    
    async def _handle_write_request(self, message: AgentMessage) -> Optional[AgentMessage]:
        """Handle write request messages (not applicable to plan reviewer)."""
        self.logger.warning("PlanReviewerAgent received write request - not applicable")
        return None
    
    async def _handle_test_request(self, message: AgentMessage) -> Optional[AgentMessage]:
        """Handle test request messages (not applicable to plan reviewer)."""
        self.logger.warning("PlanReviewerAgent received test request - not applicable")
        return None
    
    async def _handle_fix_request(self, message: AgentMessage) -> Optional[AgentMessage]:
        """Handle fix request messages (not applicable to plan reviewer)."""
        self.logger.warning("PlanReviewerAgent received fix request - not applicable")
        return None
    
    def assess_plan_quality(self, review_feedback: Dict[str, Any]) -> Dict[str, Any]:
        """
        Assess the overall quality of a plan based on review feedback.
        
        Args:
            review_feedback: The review feedback dictionary
            
        Returns:
            Quality assessment with recommendations
        """
        
        try:
            overall_assessment = review_feedback.get("overall_assessment", {})
            issues = review_feedback.get("issues", [])
            improvements = review_feedback.get("improvements", [])
            
            score = overall_assessment.get("score", 5)
            recommendation = overall_assessment.get("recommendation", "needs_major_revision")
            
            # Count critical issues
            critical_issues = len([issue for issue in issues if issue.get("severity") == "critical"])
            high_priority_improvements = len([imp for imp in improvements if imp.get("priority") == "high"])
            
            # Determine overall quality
            if score >= 8 and critical_issues == 0:
                quality = "excellent"
                action = "approve"
            elif score >= 6 and critical_issues <= 1:
                quality = "good"
                action = "approve_with_minor_changes"
            elif score >= 4:
                quality = "fair"
                action = "needs_revision"
            else:
                quality = "poor"
                action = "needs_major_revision"
            
            return {
                "quality": quality,
                "action": action,
                "score": score,
                "critical_issues": critical_issues,
                "high_priority_improvements": high_priority_improvements,
                "recommendation": recommendation
            }
            
        except Exception as e:
            self.logger.error(f"Failed to assess plan quality: {e}")
            return {
                "quality": "unknown",
                "action": "needs_review",
                "score": 0,
                "error": str(e)
            }